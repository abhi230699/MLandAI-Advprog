{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"(15-07-2020)","provenance":[],"authorship_tag":"ABX9TyPfQmUlVUuAEtbcyNktjrf3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7AdSzTv8NEY6","colab_type":"text"},"source":["### Inroduction:\n","  Aiming to minimize police response time by detecting weapons in a live cctv camera. The main motivation of this project is due to the increasing number of school mass shootings in the U.S.\n","\n","#### This notebook is a part of this [medium post](https://medium.com/@alaasinjab/detailed-tutorial-build-your-custom-real-time-object-detector-5ade1017fd2d).\n"]},{"cell_type":"code","metadata":{"id":"-WSFgpaqM5wI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594996685254,"user_tz":-330,"elapsed":2207,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"62bc92f9-0c3a-49db-8215-c6bb71bb0689"},"source":["%tensorflow_version 1.x"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CScfv6wRNkda","colab_type":"text"},"source":["### This notebook was designed to be ran from top to bottom without the need to mount Google Drive\n","### Weapon Detection Using Tensorflow Object Detection API"]},{"cell_type":"markdown","metadata":{"id":"rS-BEw7TNoVf","colab_type":"text"},"source":["Workspace structure\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TqcQadhFOOWX","colab_type":"text"},"source":["\n","    gun_detection/\n","        ├─ data/\n","        │    ├── images/\n","        │    │      ├── armas (1).jpg\n","        │    │      ├── armas (2).jpg\n","        │    │      └── ...\n","        │    ├── train_labels/\n","        │    │      ├── armas (1).xml\n","        │    │      ├── armas (2).xml\n","        │    │      └── ...\n","        │    ├── test_labels/\n","        │    │      ├── armas (10).xml\n","        │    │      ├── armas (20).xml\n","        │    │      └── ...\n","        │    ├── label_map.pbtxt\n","        │    ├── test_labels.csv\n","        │    ├── train_labels.csv\n","        │    ├── test_labels.record\n","        │    └── train_labels.record\n","        └─ models/\n","             ├─ research/\n","             │      ├── fine_tuned_model/\n","             │      │         ├── frozen_inference_graph.pb\n","             │      │         └── ...\n","             │      │         \n","             │      ├── pretrained_model/\n","             │      │         ├── frozen_inference_graph.pb\n","             │      │         └── ...\n","             │      │         \n","             │      ├── object_detection/\n","             │      │         ├── utils/\n","             │      │         ├── samples/\n","             │      │         │      ├── samples/ \n","             │      │         │      │       ├── configs/             \n","             │      │         │      │       │     ├── ssd_mobilenet_v2_coco.config\n","             │      │         │      │       │     ├── rfcn_resnet101_pets.config\n","             │      │         │      │       │     └── ...\n","             │      │         │      │       └── ... \n","             │      │         │      └── ...                                \n","             │      │         ├── export_inference_graph.py\n","             │      │         ├── model_main.py\n","             │      │         └── ...\n","             │      │         \n","             │      ├── training/\n","             │      │         ├── events.out.tfevents.xxxxx\n","             │      │         └── ...               \n","             │      └── ...\n","             └── ..."]},{"cell_type":"markdown","metadata":{"id":"aZApyCvgOkaL","colab_type":"text"},"source":["### Choosing a pre training model"]},{"cell_type":"markdown","metadata":{"id":"a9cTSrkMOn3l","colab_type":"text"},"source":["   The model used for this project is ssd_mobilenet_v2_coco. Check other models from [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models).\n","\n","\n","\n","\n","\n","  Because the interestes of this project is to interfere on real time video, i am chosing a model that has a high inference speed (ms) with relativly high mAP on COCO\n","\n"]},{"cell_type":"code","metadata":{"id":"QJr69NE3NbYZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594997077948,"user_tz":-330,"elapsed":1237,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}}},"source":["# Some models to train on\n","MODELS_CONFIG = {\n","    'ssd_mobilenet_v2': {\n","        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n","        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n","    },\n","    'faster_rcnn_inception_v2': {\n","        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n","        'pipeline_file': 'faster_rcnn_inception_v2_pets.config',\n","    },\n","    'rfcn_resnet101': {\n","        'model_name': 'rfcn_resnet101_coco_2018_01_28',\n","        'pipeline_file': 'rfcn_resnet101_pets.config',\n","    }\n","}\n","\n","# Select a model in `MODELS_CONFIG`.\n","# I chose ssd_mobilenet_v2 for this project, you could choose any\n","selected_model = 'ssd_mobilenet_v2'"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MJN-_5QfPDRI","colab_type":"text"},"source":["### Installing Required Packages"]},{"cell_type":"code","metadata":{"id":"eOdGQ5d3O_KB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":663},"executionInfo":{"status":"ok","timestamp":1594997135539,"user_tz":-330,"elapsed":25615,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"a9a2d641-f86a-4632-e880-55165de32866"},"source":["!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n","\n","!pip install -qq Cython contextlib2 pillow lxml matplotlib\n","\n","!pip install -qq pycocotools"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Selecting previously unselected package python-bs4.\n","(Reading database ... 144465 files and directories currently installed.)\n","Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n","Unpacking python-bs4 (4.6.0-1) ...\n","Selecting previously unselected package python-pkg-resources.\n","Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n","Unpacking python-pkg-resources (39.0.1-2) ...\n","Selecting previously unselected package python-chardet.\n","Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n","Unpacking python-chardet (3.0.4-1) ...\n","Selecting previously unselected package python-six.\n","Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n","Unpacking python-six (1.11.0-2) ...\n","Selecting previously unselected package python-webencodings.\n","Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n","Unpacking python-webencodings (0.5-2) ...\n","Selecting previously unselected package python-html5lib.\n","Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n","Unpacking python-html5lib (0.999999999-1) ...\n","Selecting previously unselected package python-lxml:amd64.\n","Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.1_amd64.deb ...\n","Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n","Selecting previously unselected package python-olefile.\n","Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n","Unpacking python-olefile (0.45.1-1) ...\n","Selecting previously unselected package python-pil:amd64.\n","Preparing to unpack .../8-python-pil_5.1.0-1ubuntu0.2_amd64.deb ...\n","Unpacking python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n","Setting up python-pkg-resources (39.0.1-2) ...\n","Setting up python-six (1.11.0-2) ...\n","Setting up python-bs4 (4.6.0-1) ...\n","Setting up python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n","Setting up python-olefile (0.45.1-1) ...\n","Setting up python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n","Setting up python-webencodings (0.5-2) ...\n","Setting up python-chardet (3.0.4-1) ...\n","Setting up python-html5lib (0.999999999-1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8433jFNePOGX","colab_type":"text"},"source":["### General imports"]},{"cell_type":"markdown","metadata":{"id":"o8AXYQWbPTYj","colab_type":"text"},"source":["Other Imports will be done after downloading some packages later."]},{"cell_type":"code","metadata":{"id":"FQ3QmFRuPHRX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594997192476,"user_tz":-330,"elapsed":6054,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}}},"source":["from __future__ import division, print_function, absolute_import\n","\n","import pandas as pd\n","import numpy as np\n","import csv\n","import re\n","import cv2 \n","import os\n","import glob\n","import xml.etree.ElementTree as ET\n","\n","import io\n","import tensorflow.compat.v1 as tf\n","\n","from PIL import Image\n","from collections import namedtuple, OrderedDict\n","\n","import shutil\n","import urllib.request\n","import tarfile\n","\n","from google.colab import files"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZEGLpQ5PZ6P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594997197773,"user_tz":-330,"elapsed":882,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"b1492b76-519a-4d5a-a322-e57f42749b01"},"source":["#we need tenorflow v 1.15.0, object detection API is removed from tf v 2.0+\n","#!pip install tensorflow==1.15.0\n","\n","print(tf.__version__)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["1.15.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IBSeMLS0PiFh","colab_type":"text"},"source":["## Downloading and Orgniazing Images and Annotations\n","1. Downloading the images and annotations from the source and unziping them\n","2. Creating a directory (data) to save some data such as; images, annotation, csv, etc...\n","3. Creating two directories; for the training and testing labels (not the images)\n","4. Randomly splitting our labels into 80% training and 20% testing and moving the splits to their directories: (train_labels) & (test_labels)"]},{"cell_type":"code","metadata":{"id":"IJWTF4PBPcfy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594997261605,"user_tz":-330,"elapsed":2644,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}}},"source":["#creates a directory for the whole project\n","!mkdir gun_detection"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"wdQmd0-bPrhi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594997271584,"user_tz":-330,"elapsed":1216,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"f049908d-7ba1-4f90-a69d-4b9105a15428"},"source":["cd gun_detection"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/content/gun_detection\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7MDwJAj1Pubt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":411},"executionInfo":{"status":"ok","timestamp":1594997318895,"user_tz":-330,"elapsed":33808,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"58bb3840-a1cb-4a60-a676-5f5b86e5f1e7"},"source":["#Training images and annotations\n","\n","#Source: https://sci2s.ugr.es/weapons-detection\n","\n","\n","#download the images zip\n","!wget https://sci2s.ugr.es/sites/default/files/files/TematicWebSites/WeaponsDetection/BasesDeDatos/WeaponS.zip\n","\n","#unzip the image file\n","!unzip -q WeaponS.zip\n","\n","#download the annotations zip\n","!wget https://sci2s.ugr.es/sites/default/files/files/TematicWebSites/WeaponsDetection/BasesDeDatos/WeaponS_bbox.zip\n","\n","#unzip the annotations file\n","!unzip -q WeaponS_bbox.zip"],"execution_count":8,"outputs":[{"output_type":"stream","text":["--2020-07-17 14:48:07--  https://sci2s.ugr.es/sites/default/files/files/TematicWebSites/WeaponsDetection/BasesDeDatos/WeaponS.zip\n","Resolving sci2s.ugr.es (sci2s.ugr.es)... 150.214.190.154\n","Connecting to sci2s.ugr.es (sci2s.ugr.es)|150.214.190.154|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 250005059 (238M) [application/zip]\n","Saving to: ‘WeaponS.zip’\n","\n","WeaponS.zip         100%[===================>] 238.42M  11.2MB/s    in 22s     \n","\n","2020-07-17 14:48:29 (10.8 MB/s) - ‘WeaponS.zip’ saved [250005059/250005059]\n","\n","--2020-07-17 14:48:34--  https://sci2s.ugr.es/sites/default/files/files/TematicWebSites/WeaponsDetection/BasesDeDatos/WeaponS_bbox.zip\n","Resolving sci2s.ugr.es (sci2s.ugr.es)... 150.214.190.154\n","Connecting to sci2s.ugr.es (sci2s.ugr.es)|150.214.190.154|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1420022 (1.4M) [application/zip]\n","Saving to: ‘WeaponS_bbox.zip’\n","\n","WeaponS_bbox.zip    100%[===================>]   1.35M  1.83MB/s    in 0.7s    \n","\n","2020-07-17 14:48:36 (1.83 MB/s) - ‘WeaponS_bbox.zip’ saved [1420022/1420022]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IYmPUryDPyCV","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594997332688,"user_tz":-330,"elapsed":13784,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}}},"source":["# creating a directory to store the training and testing data\n","!mkdir data\n","\n","# folders for the training and testing data.\n","!mkdir data/images data/train_labels data/test_labels\n","\n","\n","# combining the images and annotation in the training folder:\n","# moves the images to data folder\n","!mv WeaponS/* data/images\n","\n","# moves the annotations to data folder\n","!mv WeaponS_bbox/* data/train_labels"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"sYCHZtfDP2KC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594997335758,"user_tz":-330,"elapsed":3058,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"b0619194-78df-4b89-bf93-364c89f50ddf"},"source":["import os\n","os.listdir('data')"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['train_labels', 'test_labels', 'images']"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"ARukli46P4Dw","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594997342326,"user_tz":-330,"elapsed":6556,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}}},"source":["# Deleting the zipped and unzipped folders \n","!rm -rf WeaponS_bbox.zip  WeaponS.zip WeaponS/  WeaponS_bbox/"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"5sm24R6hP55R","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594997348044,"user_tz":-330,"elapsed":12255,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}}},"source":["# lists the files inside 'annotations' in a random order (not really random, by their hash value instead)\n","# Moves the first 600 labels to the testing dir: `test_labels`\n","!ls data/train_labels/* | sort -R | head -600 | xargs -I{} mv {} data/test_labels"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pgs0Qk7zP722","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594997352854,"user_tz":-330,"elapsed":17049,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"f0598909-24e7-4b87-e11a-660c8d2426ec"},"source":["# 2400 \"images\"(xml) for training\n","!ls data/train_labels/ | wc -l"],"execution_count":13,"outputs":[{"output_type":"stream","text":["2400\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"noi9gxwAP9wF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594997360408,"user_tz":-330,"elapsed":7543,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"b7bac592-1d4b-4b68-cf27-32a68e1ae639"},"source":["# 600 \"images\"(xml) for testing\n","!ls  data/test_labels/ | wc -l"],"execution_count":14,"outputs":[{"output_type":"stream","text":["600\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vHkXMrvsQCrq","colab_type":"text"},"source":["## Preprocessing Images and Labels\n","1. Converting the annotations from xml files to two csv files for each train_labels/ and train_labels/.\n","2. Creating a pbtxt file that specifies the number of class (one class in this case)\n","3. Checking if the annotations for each object are placed within the range of the image width and height."]},{"cell_type":"code","metadata":{"id":"nEwZqhGMP_y5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1594997458694,"user_tz":-330,"elapsed":958,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"3be6d341-23ae-4be3-c2eb-15a78660aeb9"},"source":["#adjusted from: https://github.com/datitran/raccoon_dataset\n","\n","#converts the annotations/labels into one csv file for each training and testing labels\n","#creats label_map.pbtxt file\n","\n","%cd /content/gun_detection/data\n","\n","\n","# images extension\n","images_extension = 'jpg'\n","\n","# takes the path of a directory that contains xml files and converts\n","#  them to one csv file.\n","\n","# returns a csv file that contains: image name, width, height, class, xmin, ymin, xmax, ymax.\n","# note: if the xml file contains more than one box/label, it will create more than one row for the same image. each row contains the info for an individual box. \n","def xml_to_csv(path):\n","  classes_names = []\n","  xml_list = []\n","\n","  for xml_file in glob.glob(path + '/*.xml'):\n","    tree = ET.parse(xml_file)\n","    root = tree.getroot()\n","    for member in root.findall('object'):\n","      classes_names.append(member[0].text)\n","      value = (root.find('filename').text + '.' + images_extension,\n","               int(root.find('size')[0].text),\n","               int(root.find('size')[1].text),\n","               member[0].text,\n","               int(member[4][0].text),\n","               int(member[4][1].text),\n","               int(member[4][2].text),\n","               int(member[4][3].text))\n","      xml_list.append(value)\n","  column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n","  xml_df = pd.DataFrame(xml_list, columns=column_name) \n","  classes_names = list(set(classes_names))\n","  classes_names.sort()\n","  return xml_df, classes_names\n","\n","# for both the train_labels and test_labels csv files, it runs the xml_to_csv() above.\n","for label_path in ['train_labels', 'test_labels']:\n","  image_path = os.path.join(os.getcwd(), label_path)\n","  xml_df, classes = xml_to_csv(label_path)\n","  xml_df.to_csv(f'{label_path}.csv', index=None)\n","  print(f'Successfully converted {label_path} xml to csv.')\n","\n","# Creating the `label_map.pbtxt` file\n","label_map_path = os.path.join(\"label_map.pbtxt\")\n","\n","pbtxt_content = \"\"\n","\n","#creats a pbtxt file the has the class names.\n","for i, class_name in enumerate(classes):\n","    # display_name is optional.\n","    pbtxt_content = (\n","        pbtxt_content\n","        + \"item {{\\n    id: {0}\\n    name: '{1}'\\n    display_name: 'Gun'\\n }}\\n\\n\".format(i + 1, class_name)\n","    )\n","pbtxt_content = pbtxt_content.strip()\n","with open(label_map_path, \"w\") as f:\n","    f.write(pbtxt_content)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["/content/gun_detection/data\n","Successfully converted train_labels xml to csv.\n","Successfully converted test_labels xml to csv.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l9U1OyrNQcLg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1594997490753,"user_tz":-330,"elapsed":3388,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"ad37ed26-4a5d-4359-9bda-954e9ab4b9b8"},"source":["#checking the pbtxt file\n","!cat label_map.pbtxt"],"execution_count":16,"outputs":[{"output_type":"stream","text":["item {\n","    id: 1\n","    name: 'pistol'\n","    display_name: 'Gun'\n"," }"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"95Vy3qG1QjaH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1594997506529,"user_tz":-330,"elapsed":3367,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"78e4b9e5-2be3-4747-e608-a9b5216cafd3"},"source":["# they are there!\n","!ls -l"],"execution_count":17,"outputs":[{"output_type":"stream","text":["total 412\n","drwxr-xr-x 2 root root 122880 Jul 17 14:48 images\n","-rw-r--r-- 1 root root     62 Jul 17 14:50 label_map.pbtxt\n","drwxr-xr-x 2 root root  20480 Jul 17 14:49 test_labels\n","-rw-r--r-- 1 root root  33111 Jul 17 14:50 test_labels.csv\n","drwxr-xr-x 2 root root 106496 Jul 17 14:49 train_labels\n","-rw-r--r-- 1 root root 126810 Jul 17 14:50 train_labels.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4MhJaNg7QnRL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"executionInfo":{"status":"ok","timestamp":1594997580897,"user_tz":-330,"elapsed":27072,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"80c33d8c-3b8c-43e5-84ef-fd5d209d1c5c"},"source":["#checks if the images box position is placed within the image.\n","\n","#note: while this doesn't checks if the boxes/annotatoins are correctly\n","# placed around the object, Tensorflow will through an error if this occured.\n","%cd /content/gun_detection/data\n","# path to images\n","images_path = 'images'\n","\n","#loops over both train_labels and test_labels csv files to do the check\n","# returns the image name where an error is found \n","# return the incorrect attributes; xmin, ymin, xmax, ymax.\n","for CSV_FILE in ['train_labels.csv', 'test_labels.csv']:\n","  with open(CSV_FILE, 'r') as fid:  \n","      print('[*] Checking file:', CSV_FILE) \n","      file = csv.reader(fid, delimiter=',')\n","      first = True \n","      cnt = 0\n","      error_cnt = 0\n","      error = False\n","      for row in file:\n","          if error == True:\n","              error_cnt += 1\n","              error = False         \n","          if first == True:\n","              first = False\n","              continue     \n","          cnt += 1      \n","          name, width, height, xmin, ymin, xmax, ymax = row[0], int(row[1]), int(row[2]), int(row[4]), int(row[5]), int(row[6]), int(row[7])     \n","          path = os.path.join(images_path, name)\n","          img = cv2.imread(path)         \n","          if type(img) == type(None):\n","              error = True\n","              print('Could not read image', img)\n","              continue     \n","          org_height, org_width = img.shape[:2]     \n","          if org_width != width:\n","              error = True\n","              print('Width mismatch for image: ', name, width, '!=', org_width)     \n","          if org_height != height:\n","              error = True\n","              print('Height mismatch for image: ', name, height, '!=', org_height) \n","          if xmin > org_width:\n","              error = True\n","              print('XMIN > org_width for file', name)  \n","          if xmax > org_width:\n","              error = True\n","              print('XMAX > org_width for file', name)\n","          if ymin > org_height:\n","              error = True\n","              print('YMIN > org_height for file', name)\n","          if ymax > org_height:\n","              error = True\n","              print('YMAX > org_height for file', name)\n","          if error == True:\n","              print('Error for file: %s' % name)\n","              print()\n","      print()\n","      print('Checked %d files and realized %d errors' % (cnt, error_cnt))\n","      print(\"-----\")"],"execution_count":18,"outputs":[{"output_type":"stream","text":["/content/gun_detection/data\n","[*] Checking file: train_labels.csv\n","XMIN > org_width for file armas (2815).jpg\n","XMAX > org_width for file armas (2815).jpg\n","YMIN > org_height for file armas (2815).jpg\n","YMAX > org_height for file armas (2815).jpg\n","Error for file: armas (2815).jpg\n","\n","\n","Checked 2748 files and realized 1 errors\n","-----\n","[*] Checking file: test_labels.csv\n","\n","Checked 716 files and realized 0 errors\n","-----\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Sp4-ZNf1Qzmt","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594997588111,"user_tz":-330,"elapsed":3227,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}}},"source":["#we have only one image with incorrect box position, we could just remove it \n","#removing the image \n","!rm images/'armas (2815).jpg'"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dy9r5FDBQ7Bn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594997605644,"user_tz":-330,"elapsed":1016,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}}},"source":["#removing the entry for it in the csv for that image as well\n","\n","#because we did a random split for the data, we dont know if it ended up being in training or testing\n","# we will remove the image from both.\n","\n","#training\n","#reading the training csv\n","df = pd.read_csv('/content/gun_detection/data/train_labels.csv')\n","# removing armas (2815).jpg\n","df = df[df['filename'] != 'armas (2815).jpg']\n","#reseting the index\n","df.reset_index(drop=True, inplace=True)\n","#saving the df\n","df.to_csv('/content/gun_detection/data/train_labels.csv')\n","\n","\n","#testing\n","#reading the testing csv\n","df = pd.read_csv('/content/gun_detection/data/test_labels.csv')\n","# removing armas (2815).jpg\n","df = df[df['filename'] != 'armas (2815).jpg']\n","#reseting the index\n","df.reset_index(drop=True, inplace=True)\n","#saving the df\n","df.to_csv('/content/gun_detection/data/test_labels.csv')\n","\n","# Just for the memory\n","df = None"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mh7BhJCERECK","colab_type":"text"},"source":["### Downloading and Preparing Tensorflow model¶\n","1. Cloning Tensorflow models from the offical git repo. The repo contains the object detection API we are interseted in.\n","2. Compiling the protos and adding folders to the os environment.\n","3. Testing the model builder."]},{"cell_type":"code","metadata":{"id":"XG2tdnw9RAC6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594997684433,"user_tz":-330,"elapsed":24536,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"8b4f7a01-7e39-4b34-c6d1-6d649b0ccf2e"},"source":["# Downlaods Tenorflow\n","%cd /content/gun_detection/\n","!git clone --q https://github.com/tensorflow/models.git"],"execution_count":21,"outputs":[{"output_type":"stream","text":["/content/gun_detection\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"coJqrORZRNiU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594997687552,"user_tz":-330,"elapsed":3113,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}}},"source":["!mv models/official models/research/official"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"RTfNUOKBRQlH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1594997690099,"user_tz":-330,"elapsed":5652,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"56bcafa3-35c3-46c7-81e4-e59f82c32f7a"},"source":["%cd /content/gun_detection/models/research\n","#compiling the proto buffers (not important to understand for this project but you can learn more about them here: https://developers.google.com/protocol-buffers/)\n","!protoc object_detection/protos/*.proto --python_out=.\n","\n","# exports the PYTHONPATH environment variable with the reasearch and slim folders' paths\n","os.environ['PYTHONPATH'] += ':/content/gun_detection/models/research/:/content/gun_detection/models/research/slim/'"],"execution_count":23,"outputs":[{"output_type":"stream","text":["/content/gun_detection/models/research\n","object_detection/protos/input_reader.proto: warning: Import object_detection/protos/image_resizer.proto but not used.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RMjHZFygRSxS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1594997712712,"user_tz":-330,"elapsed":13097,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"8833e03b-0de1-4086-b326-b95d12138fdb"},"source":["# testing the model builder\n","!pip install tf_slim\n","\n","!python3 object_detection/builders/model_builder_test.py"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Collecting tf_slim\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n","\r\u001b[K     |█                               | 10kB 16.1MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 358kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf_slim) (0.9.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.2.2->tf_slim) (1.12.0)\n","Installing collected packages: tf-slim\n","Successfully installed tf-slim-1.1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FWCQwnP4RfrR","colab_type":"text"},"source":["### Generating Tf record\n","- Generating two TFRecords files for the training and testing CSVs.\n","- Tensorflow accepts the data as tfrecords which is a binary file that run fast with low memory usage. Instead of loading the full data into memory, Tenorflow breaks the data into batches using these TFRecords automatically"]},{"cell_type":"code","metadata":{"id":"YH2jUsPLRXOr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1594997803392,"user_tz":-330,"elapsed":6461,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"8129c0b2-3bc8-4196-acfa-a62cce8f08d7"},"source":["\n","#adjusted from: https://github.com/datitran/raccoon_dataset\n","\n","# converts the csv files for training and testing data to two TFRecords files.\n","# places the output in the same directory as the input\n","\n","\n","from object_detection.utils import dataset_util\n","%cd /content/gun_detection/models/\n","\n","DATA_BASE_PATH = '/content/gun_detection/data/'\n","image_dir = DATA_BASE_PATH +'images/'\n","\n","def class_text_to_int(row_label):\n","\t\tif row_label == 'pistol':\n","\t\t\t\treturn 1\n","\t\telse:\n","\t\t\t\tNone\n","\n","\n","def split(df, group):\n","\t\tdata = namedtuple('data', ['filename', 'object'])\n","\t\tgb = df.groupby(group)\n","\t\treturn [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n","\n","def create_tf_example(group, path):\n","\t\twith tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n","\t\t\t\tencoded_jpg = fid.read()\n","\t\tencoded_jpg_io = io.BytesIO(encoded_jpg)\n","\t\timage = Image.open(encoded_jpg_io)\n","\t\twidth, height = image.size\n","\n","\t\tfilename = group.filename.encode('utf8')\n","\t\timage_format = b'jpg'\n","\t\txmins = []\n","\t\txmaxs = []\n","\t\tymins = []\n","\t\tymaxs = []\n","\t\tclasses_text = []\n","\t\tclasses = []\n","\n","\t\tfor index, row in group.object.iterrows():\n","\t\t\t\txmins.append(row['xmin'] / width)\n","\t\t\t\txmaxs.append(row['xmax'] / width)\n","\t\t\t\tymins.append(row['ymin'] / height)\n","\t\t\t\tymaxs.append(row['ymax'] / height)\n","\t\t\t\tclasses_text.append(row['class'].encode('utf8'))\n","\t\t\t\tclasses.append(class_text_to_int(row['class']))\n","\n","\t\ttf_example = tf.train.Example(features=tf.train.Features(feature={\n","\t\t\t\t'image/height': dataset_util.int64_feature(height),\n","\t\t\t\t'image/width': dataset_util.int64_feature(width),\n","\t\t\t\t'image/filename': dataset_util.bytes_feature(filename),\n","\t\t\t\t'image/source_id': dataset_util.bytes_feature(filename),\n","\t\t\t\t'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n","\t\t\t\t'image/format': dataset_util.bytes_feature(image_format),\n","\t\t\t\t'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n","\t\t\t\t'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n","\t\t\t\t'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n","\t\t\t\t'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n","\t\t\t\t'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n","\t\t\t\t'image/object/class/label': dataset_util.int64_list_feature(classes),\n","\t\t}))\n","\t\treturn tf_example\n","\n","for csv in ['train_labels', 'test_labels']:\n","  writer = tf.io.TFRecordWriter(DATA_BASE_PATH + csv + '.record')\n","  path = os.path.join(image_dir)\n","  examples = pd.read_csv(DATA_BASE_PATH + csv + '.csv')\n","  grouped = split(examples, 'filename')\n","  for group in grouped:\n","      tf_example = create_tf_example(group, path)\n","      writer.write(tf_example.SerializeToString())\n","    \n","  writer.close()\n","  output_path = os.path.join(os.getcwd(), DATA_BASE_PATH + csv + '.record')\n","  print('Successfully created the TFRecords: {}'.format(DATA_BASE_PATH +csv + '.record'))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["/content/gun_detection/models\n","Successfully created the TFRecords: /content/gun_detection/data/train_labels.record\n","Successfully created the TFRecords: /content/gun_detection/data/test_labels.record\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cx4TW2KuRu-2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1594997812987,"user_tz":-330,"elapsed":2511,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"cb818008-fdf2-4634-9eb9-6ea0f3cc7e24"},"source":["# TFRecords are created\n","!ls -lX /content/gun_detection/data/"],"execution_count":26,"outputs":[{"output_type":"stream","text":["total 252476\n","drwxr-xr-x 2 root root    122880 Jul 17 14:53 images\n","drwxr-xr-x 2 root root     20480 Jul 17 14:49 test_labels\n","drwxr-xr-x 2 root root    106496 Jul 17 14:49 train_labels\n","-rw-r--r-- 1 root root     35866 Jul 17 14:53 test_labels.csv\n","-rw-r--r-- 1 root root    139337 Jul 17 14:53 train_labels.csv\n","-rw-r--r-- 1 root root        62 Jul 17 14:50 label_map.pbtxt\n","-rw-r--r-- 1 root root  54695539 Jul 17 14:56 test_labels.record\n","-rw-r--r-- 1 root root 203398615 Jul 17 14:56 train_labels.record\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZpB8wHb8R771","colab_type":"text"},"source":["### Downloading the Base Model\n","1. Based on the model selecting at the top of this notebook, downloading the model selected and extracting its content.\n","2. Creating a dir to save the model while training."]},{"cell_type":"code","metadata":{"id":"Ragiu_bfRySa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594997876644,"user_tz":-330,"elapsed":4741,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"b78f4515-c9aa-4c87-98ac-25b6aa90c59c"},"source":["%cd /content/gun_detection/models/research\n","\n","# Name of the object detection model to use.\n","MODEL = MODELS_CONFIG[selected_model]['model_name']\n","\n","# Name of the pipline file in tensorflow object detection API.\n","pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n","\n","#selecting the model\n","MODEL_FILE = MODEL + '.tar.gz'\n","\n","#creating the downlaod link for the model selected\n","DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n","\n","#the distination folder where the model will be saved\n","fine_tune_dir = '/content/gun_detection/models/research/pretrained_model'\n","\n","#checks if the model has already been downloaded\n","if not (os.path.exists(MODEL_FILE)):\n","    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n","\n","#unzipping the file and extracting its content\n","tar = tarfile.open(MODEL_FILE)\n","tar.extractall()\n","tar.close()\n","\n","# creating an output file to save the model while training\n","os.remove(MODEL_FILE)\n","if (os.path.exists(fine_tune_dir)):\n","    shutil.rmtree(fine_tune_dir)\n","os.rename(MODEL, fine_tune_dir)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["/content/gun_detection/models/research\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lPFhElifR48H","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1594997888808,"user_tz":-330,"elapsed":5083,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"b106a4d6-ee4d-43f9-fa4b-97780045c833"},"source":["#checking the content of the pretrained model.\n","# this is the directory of the \"fine_tune_checkpoint\" that is used in the config file.\n","!echo {fine_tune_dir}\n","!ls -alh {fine_tune_dir}"],"execution_count":29,"outputs":[{"output_type":"stream","text":["/content/gun_detection/models/research/pretrained_model\n","total 135M\n","drwxr-xr-x  3 345018 89939 4.0K Mar 30  2018 .\n","drwxr-xr-x 64 root   root  4.0K Jul 17 14:57 ..\n","-rw-r--r--  1 345018 89939   77 Mar 30  2018 checkpoint\n","-rw-r--r--  1 345018 89939  67M Mar 30  2018 frozen_inference_graph.pb\n","-rw-r--r--  1 345018 89939  65M Mar 30  2018 model.ckpt.data-00000-of-00001\n","-rw-r--r--  1 345018 89939  15K Mar 30  2018 model.ckpt.index\n","-rw-r--r--  1 345018 89939 3.4M Mar 30  2018 model.ckpt.meta\n","-rw-r--r--  1 345018 89939 4.2K Mar 30  2018 pipeline.config\n","drwxr-xr-x  3 345018 89939 4.0K Mar 30  2018 saved_model\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QYfea5sBSHQv","colab_type":"text"},"source":["### Configuring the Training Pipeline\n","1. Adding the path for the TFRecords files and pbtxt,batch_size,num_steps,num_classes to the configuration file.\n","2. Adding some Image augmentation.\n","3. Creating a directory to save the model at each checkpoint while training."]},{"cell_type":"code","metadata":{"id":"89yD6JUgSELs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1594997930882,"user_tz":-330,"elapsed":904,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"373d904f-8cc4-45c3-d2df-66bc2d6f4ff4"},"source":["#the path to the folder containing all the sample config files\n","CONFIG_BASE = \"/content/gun_detection/models/research/object_detection/samples/configs/\"\n","\n","#path to the specified model's config file\n","model_pipline = os.path.join(CONFIG_BASE, pipeline_file)\n","model_pipline\n"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic":{"type":"string"},"text/plain":["'/content/gun_detection/models/research/object_detection/samples/configs/ssd_mobilenet_v2_coco.config'"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"JhsSp47GSPeB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594997943535,"user_tz":-330,"elapsed":3614,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"f3f2a415-787a-486f-b49d-a2ef97124e33"},"source":["#check the sample config file that is provided by the tf model\n","!cat /content/gun_detection/models/research/object_detection/samples/configs/ssd_mobilenet_v2_coco.config"],"execution_count":31,"outputs":[{"output_type":"stream","text":["# SSD with Mobilenet v2 configuration for MSCOCO Dataset.\n","# Users should configure the fine_tune_checkpoint field in the train config as\n","# well as the label_map_path and input_path fields in the train_input_reader and\n","# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n","# should be configured.\n","\n","model {\n","  ssd {\n","    num_classes: 90\n","    box_coder {\n","      faster_rcnn_box_coder {\n","        y_scale: 10.0\n","        x_scale: 10.0\n","        height_scale: 5.0\n","        width_scale: 5.0\n","      }\n","    }\n","    matcher {\n","      argmax_matcher {\n","        matched_threshold: 0.5\n","        unmatched_threshold: 0.5\n","        ignore_thresholds: false\n","        negatives_lower_than_unmatched: true\n","        force_match_for_each_row: true\n","      }\n","    }\n","    similarity_calculator {\n","      iou_similarity {\n","      }\n","    }\n","    anchor_generator {\n","      ssd_anchor_generator {\n","        num_layers: 6\n","        min_scale: 0.2\n","        max_scale: 0.95\n","        aspect_ratios: 1.0\n","        aspect_ratios: 2.0\n","        aspect_ratios: 0.5\n","        aspect_ratios: 3.0\n","        aspect_ratios: 0.3333\n","      }\n","    }\n","    image_resizer {\n","      fixed_shape_resizer {\n","        height: 300\n","        width: 300\n","      }\n","    }\n","    box_predictor {\n","      convolutional_box_predictor {\n","        min_depth: 0\n","        max_depth: 0\n","        num_layers_before_predictor: 0\n","        use_dropout: false\n","        dropout_keep_probability: 0.8\n","        kernel_size: 1\n","        box_code_size: 4\n","        apply_sigmoid_to_scores: false\n","        conv_hyperparams {\n","          activation: RELU_6,\n","          regularizer {\n","            l2_regularizer {\n","              weight: 0.00004\n","            }\n","          }\n","          initializer {\n","            truncated_normal_initializer {\n","              stddev: 0.03\n","              mean: 0.0\n","            }\n","          }\n","          batch_norm {\n","            train: true,\n","            scale: true,\n","            center: true,\n","            decay: 0.9997,\n","            epsilon: 0.001,\n","          }\n","        }\n","      }\n","    }\n","    feature_extractor {\n","      type: 'ssd_mobilenet_v2'\n","      min_depth: 16\n","      depth_multiplier: 1.0\n","      conv_hyperparams {\n","        activation: RELU_6,\n","        regularizer {\n","          l2_regularizer {\n","            weight: 0.00004\n","          }\n","        }\n","        initializer {\n","          truncated_normal_initializer {\n","            stddev: 0.03\n","            mean: 0.0\n","          }\n","        }\n","        batch_norm {\n","          train: true,\n","          scale: true,\n","          center: true,\n","          decay: 0.9997,\n","          epsilon: 0.001,\n","        }\n","      }\n","    }\n","    loss {\n","      classification_loss {\n","        weighted_sigmoid {\n","        }\n","      }\n","      localization_loss {\n","        weighted_smooth_l1 {\n","        }\n","      }\n","      hard_example_miner {\n","        num_hard_examples: 3000\n","        iou_threshold: 0.99\n","        loss_type: CLASSIFICATION\n","        max_negatives_per_positive: 3\n","        min_negatives_per_image: 3\n","      }\n","      classification_weight: 1.0\n","      localization_weight: 1.0\n","    }\n","    normalize_loss_by_num_matches: true\n","    post_processing {\n","      batch_non_max_suppression {\n","        score_threshold: 1e-8\n","        iou_threshold: 0.6\n","        max_detections_per_class: 100\n","        max_total_detections: 100\n","      }\n","      score_converter: SIGMOID\n","    }\n","  }\n","}\n","\n","train_config: {\n","  batch_size: 24\n","  optimizer {\n","    rms_prop_optimizer: {\n","      learning_rate: {\n","        exponential_decay_learning_rate {\n","          initial_learning_rate: 0.004\n","          decay_steps: 800720\n","          decay_factor: 0.95\n","        }\n","      }\n","      momentum_optimizer_value: 0.9\n","      decay: 0.9\n","      epsilon: 1.0\n","    }\n","  }\n","  fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/model.ckpt\"\n","  fine_tune_checkpoint_type:  \"detection\"\n","  # Note: The below line limits the training process to 200K steps, which we\n","  # empirically found to be sufficient enough to train the pets dataset. This\n","  # effectively bypasses the learning rate schedule (the learning rate will\n","  # never decay). Remove the below line to train indefinitely.\n","  num_steps: 200000\n","  data_augmentation_options {\n","    random_horizontal_flip {\n","    }\n","  }\n","  data_augmentation_options {\n","    ssd_random_crop {\n","    }\n","  }\n","}\n","\n","train_input_reader: {\n","  tf_record_input_reader {\n","    input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100\"\n","  }\n","  label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\n","}\n","\n","eval_config: {\n","  num_examples: 8000\n","  # Note: The below line limits the evaluation process to 10 evaluations.\n","  # Remove the below line to evaluate indefinitely.\n","  max_evals: 10\n","}\n","\n","eval_input_reader: {\n","  tf_record_input_reader {\n","    input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010\"\n","  }\n","  label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\n","  shuffle: false\n","  num_readers: 1\n","}"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RkyROCLTSR5e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1594997993691,"user_tz":-330,"elapsed":2325,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"5dcdfad7-1a4a-4d9d-bdc9-c5814ba10c48"},"source":["#editing the configuration file to add the path for the TFRecords files, pbtxt,batch_size,num_steps,num_classes.\n","# any image augmentation, hyperparemeter tunning (drop out, batch normalization... etc) would be editted here\n","\n","%%writefile {model_pipline}\n","model {\n","  ssd {\n","    num_classes: 1 # number of classes to be detected\n","    box_coder {\n","      faster_rcnn_box_coder {\n","        y_scale: 10.0\n","        x_scale: 10.0\n","        height_scale: 5.0\n","        width_scale: 5.0\n","      }\n","    }\n","    matcher {\n","      argmax_matcher {\n","        matched_threshold: 0.5\n","        unmatched_threshold: 0.5\n","        ignore_thresholds: false\n","        negatives_lower_than_unmatched: true\n","        force_match_for_each_row: true\n","      }\n","    }\n","    similarity_calculator {\n","      iou_similarity {\n","      }\n","    }\n","    anchor_generator {\n","      ssd_anchor_generator {\n","        num_layers: 6\n","        min_scale: 0.2\n","        max_scale: 0.95\n","        aspect_ratios: 1.0\n","        aspect_ratios: 2.0\n","        aspect_ratios: 0.5\n","        aspect_ratios: 3.0\n","        aspect_ratios: 0.3333\n","      }\n","    }\n","    # all images will be resized to the below W x H.\n","    image_resizer { \n","      fixed_shape_resizer {\n","        height: 300\n","        width: 300\n","      }\n","    }\n","    box_predictor {\n","      convolutional_box_predictor {\n","        min_depth: 0\n","        max_depth: 0\n","        num_layers_before_predictor: 0\n","        #use_dropout: false\n","        use_dropout: true # to counter over fitting. you can also try tweaking its probability below\n","        dropout_keep_probability: 0.8\n","        kernel_size: 1\n","        box_code_size: 4\n","        apply_sigmoid_to_scores: false\n","        conv_hyperparams {\n","          activation: RELU_6,\n","          regularizer {\n","            l2_regularizer {\n","            # weight: 0.00004\n","            weight: 0.001 # higher regularizition to counter overfitting\n","          }\n","          }\n","          initializer {\n","            truncated_normal_initializer {\n","              stddev: 0.03\n","              mean: 0.0\n","            }\n","          }\n","          batch_norm {\n","            train: true,\n","            scale: true,\n","            center: true,\n","            decay: 0.9997,\n","            epsilon: 0.001,\n","          }\n","        }\n","      }\n","    }\n","    feature_extractor {\n","      type: 'ssd_mobilenet_v2'\n","      min_depth: 16\n","      depth_multiplier: 1.0\n","      conv_hyperparams {\n","        activation: RELU_6,\n","        regularizer {\n","          l2_regularizer {\n","            # weight: 0.00004\n","            weight: 0.001 # higher regularizition to counter overfitting\n","          }\n","        }\n","        initializer {\n","          truncated_normal_initializer {\n","            stddev: 0.03\n","            mean: 0.0\n","          }\n","        }\n","        batch_norm {\n","          train: true,\n","          scale: true,\n","          center: true,\n","          decay: 0.9997,\n","          epsilon: 0.001,\n","        }\n","      }\n","    }\n","    loss {\n","      classification_loss {\n","        weighted_sigmoid {\n","        }\n","      }\n","      localization_loss {\n","        weighted_smooth_l1 {\n","        }\n","      }\n","      hard_example_miner {\n","        num_hard_examples: 3000 \n","        iou_threshold: 0.95\n","        loss_type: CLASSIFICATION\n","        max_negatives_per_positive: 3\n","        min_negatives_per_image: 3\n","      }\n","      classification_weight: 1.0\n","      localization_weight: 1.0\n","    }\n","    normalize_loss_by_num_matches: true\n","    post_processing {\n","      batch_non_max_suppression {\n","        score_threshold: 1e-8\n","        iou_threshold: 0.6\n","        \n","        #adjust this to the max number of objects per class. \n","        # ex, in my case, i have one pistol in most of the images.\n","        # . there are some images with more than one up to 16.\n","        max_detections_per_class: 16\n","        # max number of detections among all classes. I have 1 class only so\n","        max_total_detections: 16\n","      }\n","      score_converter: SIGMOID\n","    }\n","  }\n","}\n","\n","train_config: {\n","  batch_size: 16 # training batch size\n","  optimizer {\n","    rms_prop_optimizer: {\n","      learning_rate: {\n","        exponential_decay_learning_rate {\n","          initial_learning_rate: 0.003\n","          decay_steps: 800720\n","          decay_factor: 0.95\n","        }\n","      }\n","      momentum_optimizer_value: 0.9\n","      decay: 0.9\n","      epsilon: 1.0\n","    }\n","  }\n","\n","  #the path to the pretrained model. \n","  fine_tune_checkpoint: \"/content/gun_detection/models/research/pretrained_model/model.ckpt\"\n","  fine_tune_checkpoint_type:  \"detection\"\n","  # Note: The below line limits the training process to 200K steps, which we\n","  # empirically found to be sufficient enough to train the pets dataset. This\n","  # effectively bypasses the learning rate schedule (the learning rate will\n","  # never decay). Remove the below line to train indefinitely.\n","  num_steps: 200000 \n","  \n","\n","  #data augmentaion is done here, you can remove or add more.\n","  # They will help the model generalize but the training time will increase greatly by using more data augmentation.\n","  # Check this link to add more image augmentation: https://github.com/tensorflow/models/blob/master/research/object_detection/protos/preprocessor.proto\n","  \n","  data_augmentation_options {\n","    random_horizontal_flip {\n","    }\n","  }\n","  data_augmentation_options {\n","    random_adjust_contrast {\n","    }\n","  }\n","  data_augmentation_options {\n","    ssd_random_crop {\n","    }\n","  }\n","}\n","\n","train_input_reader: {\n","  tf_record_input_reader {\n","    #path to the training TFRecord\n","    input_path: \"/content/gun_detection/data/train_labels.record\"\n","  }\n","  #path to the label map \n","  label_map_path: \"/content/gun_detection/data/label_map.pbtxt\"\n","}\n","\n","eval_config: {\n","  # the number of images in your \"testing\" data (was 600 but we removed one above :) )\n","  num_examples: 599\n","  # the number of images to disply in Tensorboard while training\n","  num_visualizations: 20\n","\n","  # Note: The below line limits the evaluation process to 10 evaluations.\n","  # Remove the below line to evaluate indefinitely.\n","  #max_evals: 10\n","}\n","\n","eval_input_reader: {\n","  tf_record_input_reader {\n","      \n","    #path to the testing TFRecord\n","    input_path: \"/content/gun_detection/data/test_labels.record\"\n","  }\n","  #path to the label map \n","  label_map_path: \"/content/gun_detection/data/label_map.pbtxt\"\n","  shuffle: false\n","  num_readers: 1\n","}"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Overwriting /content/gun_detection/models/research/object_detection/samples/configs/ssd_mobilenet_v2_coco.config\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hAqi37wkSecT","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594998011823,"user_tz":-330,"elapsed":3247,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}}},"source":["# where the model will be saved at each checkpoint while training \n","model_dir = 'training/'\n","\n","# Optionally: remove content in output model directory to fresh start.\n","!rm -rf {model_dir}\n","os.makedirs(model_dir, exist_ok=True)\n"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jNIHMakWSlDa","colab_type":"text"},"source":["### Tensorboard\n","1. Downlaoding and unzipping Tensorboard\n","2. creating a link to visualize multiple graph while training.\n","#### notes:\n","\n","1. Tensorboard will not log any files until the training starts.\n","2. a max of 20 connection per minute is allowed when using ngrok, you will not be able to access tensorboard while the model is logging."]},{"cell_type":"code","metadata":{"id":"EiMeKX92SiqM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":258},"executionInfo":{"status":"ok","timestamp":1594998066721,"user_tz":-330,"elapsed":5892,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"56c2d7d4-013b-4871-c2bd-e3e5a373eb62"},"source":["#downlaoding ngrok to be able to access tensorboard on google colab\n","!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip -o ngrok-stable-linux-amd64.zip"],"execution_count":34,"outputs":[{"output_type":"stream","text":["--2020-07-17 15:01:03--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 54.86.229.42, 52.201.33.182, 54.208.57.0, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|54.86.229.42|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13773305 (13M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip’\n","\n","\r          ngrok-sta   0%[                    ]       0  --.-KB/s               \r         ngrok-stab  88%[================>   ]  11.66M  58.2MB/s               \rngrok-stable-linux- 100%[===================>]  13.13M  61.2MB/s    in 0.2s    \n","\n","2020-07-17 15:01:03 (61.2 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8qukLK_dSvaa","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594998072318,"user_tz":-330,"elapsed":611,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}}},"source":["#the logs that are created while training \n","LOG_DIR = model_dir\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")\n","get_ipython().system_raw('./ngrok http 6006 &')"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"h6wWktAVSyBt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594998083519,"user_tz":-330,"elapsed":3036,"user":{"displayName":"MUDDALA ABHISHEK NAIDU MUDDALA ABHISHEK NAIDU","photoUrl":"","userId":"01697563523189429538"}},"outputId":"e0d4a1ec-7e76-4f43-b0cc-c58b6a6bdbcc"},"source":["#The link to tensorboard.\n","#works after the training starts.\n","\n","### note: if you didnt get a link as output, rerun this cell and the one above\n","!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":36,"outputs":[{"output_type":"stream","text":["https://4b7ad737f3fc.ngrok.io\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X95vS9myS24X","colab_type":"text"},"source":["## Training\n","Finally training the model!"]},{"cell_type":"code","metadata":{"id":"Fad1y2EmS0Nw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"1eaac642-5143-466c-afc6-205e22e05c19"},"source":["!python3 /content/gun_detection/models/research/object_detection/model_main.py \\\n","    --pipeline_config_path={model_pipline}\\\n","    --model_dir={model_dir} \\\n","    --alsologtostderr \\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n","W0717 15:01:56.163897 140674921572224 model_lib.py:758] Forced number of epochs for all eval validations to be 1.\n","INFO:tensorflow:Maybe overwriting train_steps: None\n","I0717 15:01:56.164176 140674921572224 config_util.py:552] Maybe overwriting train_steps: None\n","INFO:tensorflow:Maybe overwriting use_bfloat16: False\n","I0717 15:01:56.164284 140674921572224 config_util.py:552] Maybe overwriting use_bfloat16: False\n","INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: 1\n","I0717 15:01:56.164436 140674921572224 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: 1\n","INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n","I0717 15:01:56.164547 140674921572224 config_util.py:552] Maybe overwriting eval_num_epochs: 1\n","WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n","W0717 15:01:56.164688 140674921572224 model_lib.py:774] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n","INFO:tensorflow:create_estimator_and_inputs: use_tpu False, export_to_tpu None\n","I0717 15:01:56.164801 140674921572224 model_lib.py:809] create_estimator_and_inputs: use_tpu False, export_to_tpu None\n","INFO:tensorflow:Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff1060c1f28>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","I0717 15:01:56.165398 140674921572224 estimator.py:212] Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff1060c1f28>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7ff1060d01e0>) includes params argument, but params are not passed to Estimator.\n","W0717 15:01:56.166329 140674921572224 model_fn.py:630] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7ff1060d01e0>) includes params argument, but params are not passed to Estimator.\n","INFO:tensorflow:Not using Distribute Coordinator.\n","I0717 15:01:56.167165 140674921572224 estimator_training.py:186] Not using Distribute Coordinator.\n","INFO:tensorflow:Running training and evaluation locally (non-distributed).\n","I0717 15:01:56.167404 140674921572224 training.py:612] Running training and evaluation locally (non-distributed).\n","INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n","I0717 15:01:56.167673 140674921572224 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","W0717 15:01:56.185178 140674921572224 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n","W0717 15:01:56.225822 140674921572224 dataset_builder.py:83] num_readers has been reduced to 1 to match input file shards.\n","WARNING:tensorflow:From /content/gun_detection/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n","W0717 15:01:56.232195 140674921572224 deprecation.py:323] From /content/gun_detection/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n","WARNING:tensorflow:From /content/gun_detection/models/research/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map()\n","W0717 15:01:56.263265 140674921572224 deprecation.py:323] From /content/gun_detection/models/research/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map()\n","WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7ff105b0ba20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n","W0717 15:01:56.301904 140674921572224 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7ff105b0ba20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7ff12bc4d268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","W0717 15:01:56.540888 140674921572224 ag_logging.py:146] Entity <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7ff12bc4d268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:From /content/gun_detection/models/research/object_detection/inputs.py:80: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n","W0717 15:01:56.548269 140674921572224 deprecation.py:323] From /content/gun_detection/models/research/object_detection/inputs.py:80: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n","WARNING:tensorflow:From /content/gun_detection/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0717 15:01:56.556244 140674921572224 deprecation.py:323] From /content/gun_detection/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/gun_detection/models/research/object_detection/core/preprocessor.py:199: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n","W0717 15:01:56.685841 140674921572224 deprecation.py:323] From /content/gun_detection/models/research/object_detection/core/preprocessor.py:199: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n","WARNING:tensorflow:From /content/gun_detection/models/research/object_detection/inputs.py:261: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","W0717 15:01:57.715874 140674921572224 deprecation.py:323] From /content/gun_detection/models/research/object_detection/inputs.py:261: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","INFO:tensorflow:Calling model_fn.\n","I0717 15:01:58.270205 140674921572224 estimator.py:1148] Calling model_fn.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","W0717 15:01:58.556809 140674921572224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:02:01.654101 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:02:01.699119 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:02:01.742246 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:02:01.786237 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:02:01.831475 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:02:01.880568 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W0717 15:02:07.314659 140674921572224 deprecation.py:506] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","INFO:tensorflow:Done calling model_fn.\n","I0717 15:02:14.182977 140674921572224 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","I0717 15:02:14.184732 140674921572224 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","I0717 15:02:17.876680 140674921572224 monitored_session.py:240] Graph was finalized.\n","2020-07-17 15:02:17.892924: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n","2020-07-17 15:02:17.893252: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15d0a1c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-07-17 15:02:17.893293: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-07-17 15:02:17.930417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-07-17 15:02:18.001255: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","2020-07-17 15:02:18.001368: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (5d409ccd339c): /proc/driver/nvidia/version does not exist\n","INFO:tensorflow:Running local_init_op.\n","I0717 15:02:22.796272 140674921572224 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0717 15:02:23.223081 140674921572224 session_manager.py:502] Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into training/model.ckpt.\n","I0717 15:02:33.239618 140674921572224 basic_session_run_hooks.py:606] Saving checkpoints for 0 into training/model.ckpt.\n","INFO:tensorflow:loss = 14.876956, step = 1\n","I0717 15:02:55.220587 140674921572224 basic_session_run_hooks.py:262] loss = 14.876956, step = 1\n","INFO:tensorflow:Saving checkpoints for 71 into training/model.ckpt.\n","I0717 15:12:35.765685 140674921572224 basic_session_run_hooks.py:606] Saving checkpoints for 71 into training/model.ckpt.\n","WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7ff0f9f82438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n","W0717 15:12:37.386645 140674921572224 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7ff0f9f82438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7ff0fed5ee18> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","W0717 15:12:37.582157 140674921572224 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7ff0fed5ee18> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","INFO:tensorflow:Calling model_fn.\n","I0717 15:12:38.174638 140674921572224 estimator.py:1148] Calling model_fn.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:12:40.645313 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:12:40.682341 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:12:40.719408 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:12:40.756244 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:12:40.805099 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:12:40.848776 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","WARNING:tensorflow:From /content/gun_detection/models/research/object_detection/eval_util.py:855: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","W0717 15:12:41.617919 140674921572224 deprecation.py:323] From /content/gun_detection/models/research/object_detection/eval_util.py:855: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","WARNING:tensorflow:From /content/gun_detection/models/research/object_detection/utils/visualization_utils.py:618: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","tf.py_func is deprecated in TF V2. Instead, there are two\n","    options available in V2.\n","    - tf.py_function takes a python function which manipulates tf eager\n","    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n","    an ndarray (just call tensor.numpy()) but having access to eager tensors\n","    means `tf.py_function`s can use accelerators such as GPUs as well as\n","    being differentiable using a gradient tape.\n","    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n","    (it is not differentiable, and manipulates numpy arrays). It drops the\n","    stateful argument making all functions stateful.\n","    \n","W0717 15:12:41.843610 140674921572224 deprecation.py:323] From /content/gun_detection/models/research/object_detection/utils/visualization_utils.py:618: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","tf.py_func is deprecated in TF V2. Instead, there are two\n","    options available in V2.\n","    - tf.py_function takes a python function which manipulates tf eager\n","    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n","    an ndarray (just call tensor.numpy()) but having access to eager tensors\n","    means `tf.py_function`s can use accelerators such as GPUs as well as\n","    being differentiable using a gradient tape.\n","    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n","    (it is not differentiable, and manipulates numpy arrays). It drops the\n","    stateful argument making all functions stateful.\n","    \n","INFO:tensorflow:Done calling model_fn.\n","I0717 15:12:42.491828 140674921572224 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2020-07-17T15:12:42Z\n","I0717 15:12:42.510038 140674921572224 evaluation.py:255] Starting evaluation at 2020-07-17T15:12:42Z\n","INFO:tensorflow:Graph was finalized.\n","I0717 15:12:42.956300 140674921572224 monitored_session.py:240] Graph was finalized.\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-71\n","I0717 15:12:42.958110 140674921572224 saver.py:1284] Restoring parameters from training/model.ckpt-71\n","INFO:tensorflow:Running local_init_op.\n","I0717 15:12:44.017404 140674921572224 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0717 15:12:44.201399 140674921572224 session_manager.py:502] Done running local_init_op.\n","INFO:tensorflow:Performing evaluation on 600 images.\n","I0717 15:14:18.203072 140672979662592 coco_evaluation.py:237] Performing evaluation on 600 images.\n","creating index...\n","index created!\n","INFO:tensorflow:Loading and preparing annotation results...\n","I0717 15:14:18.205673 140672979662592 coco_tools.py:116] Loading and preparing annotation results...\n","INFO:tensorflow:DONE (t=0.01s)\n","I0717 15:14:18.214406 140672979662592 coco_tools.py:138] DONE (t=0.01s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=1.18s).\n","Accumulating evaluation results...\n","DONE (t=0.15s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.107\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.189\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.103\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.142\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.264\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.311\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.006\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.419\n","INFO:tensorflow:Finished evaluation at 2020-07-17-15:14:19\n","I0717 15:14:19.658930 140674921572224 evaluation.py:275] Finished evaluation at 2020-07-17-15:14:19\n","INFO:tensorflow:Saving dict for global step 71: DetectionBoxes_Precision/mAP = 0.106884286, DetectionBoxes_Precision/mAP (large) = 0.1417037, DetectionBoxes_Precision/mAP (medium) = 2.0022766e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.1885598, DetectionBoxes_Precision/mAP@.75IOU = 0.10250205, DetectionBoxes_Recall/AR@1 = 0.26438546, DetectionBoxes_Recall/AR@10 = 0.31075418, DetectionBoxes_Recall/AR@100 = 0.31578213, DetectionBoxes_Recall/AR@100 (large) = 0.41858736, DetectionBoxes_Recall/AR@100 (medium) = 0.00647482, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 5.1068463, Loss/localization_loss = 2.3992882, Loss/regularization_loss = 1.0984123, Loss/total_loss = 8.604555, global_step = 71, learning_rate = 0.003, loss = 8.604555\n","I0717 15:14:19.659328 140674921572224 estimator.py:2049] Saving dict for global step 71: DetectionBoxes_Precision/mAP = 0.106884286, DetectionBoxes_Precision/mAP (large) = 0.1417037, DetectionBoxes_Precision/mAP (medium) = 2.0022766e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.1885598, DetectionBoxes_Precision/mAP@.75IOU = 0.10250205, DetectionBoxes_Recall/AR@1 = 0.26438546, DetectionBoxes_Recall/AR@10 = 0.31075418, DetectionBoxes_Recall/AR@100 = 0.31578213, DetectionBoxes_Recall/AR@100 (large) = 0.41858736, DetectionBoxes_Recall/AR@100 (medium) = 0.00647482, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 5.1068463, Loss/localization_loss = 2.3992882, Loss/regularization_loss = 1.0984123, Loss/total_loss = 8.604555, global_step = 71, learning_rate = 0.003, loss = 8.604555\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 71: training/model.ckpt-71\n","I0717 15:14:20.599569 140674921572224 estimator.py:2109] Saving 'checkpoint_path' summary for global step 71: training/model.ckpt-71\n","INFO:tensorflow:global_step/sec: 0.107231\n","I0717 15:18:27.786554 140674921572224 basic_session_run_hooks.py:692] global_step/sec: 0.107231\n","INFO:tensorflow:loss = 8.64985, step = 101 (932.567 sec)\n","I0717 15:18:27.787787 140674921572224 basic_session_run_hooks.py:260] loss = 8.64985, step = 101 (932.567 sec)\n","INFO:tensorflow:Saving checkpoints for 132 into training/model.ckpt.\n","I0717 15:22:41.489796 140674921572224 basic_session_run_hooks.py:606] Saving checkpoints for 132 into training/model.ckpt.\n","WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7ff0f38c5828>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n","W0717 15:22:43.022833 140674921572224 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7ff0f38c5828>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7ff0efa3e840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","W0717 15:22:43.219008 140674921572224 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7ff0efa3e840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","INFO:tensorflow:Calling model_fn.\n","I0717 15:22:43.783104 140674921572224 estimator.py:1148] Calling model_fn.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:22:46.218133 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:22:46.255206 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:22:46.290956 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:22:46.327573 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:22:46.364431 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:22:46.402872 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:Done calling model_fn.\n","I0717 15:22:48.041861 140674921572224 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2020-07-17T15:22:48Z\n","I0717 15:22:48.060482 140674921572224 evaluation.py:255] Starting evaluation at 2020-07-17T15:22:48Z\n","INFO:tensorflow:Graph was finalized.\n","I0717 15:22:48.506537 140674921572224 monitored_session.py:240] Graph was finalized.\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-132\n","I0717 15:22:48.508523 140674921572224 saver.py:1284] Restoring parameters from training/model.ckpt-132\n","INFO:tensorflow:Running local_init_op.\n","I0717 15:22:49.532373 140674921572224 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0717 15:22:49.715206 140674921572224 session_manager.py:502] Done running local_init_op.\n","INFO:tensorflow:Performing evaluation on 600 images.\n","I0717 15:24:22.413511 140672979662592 coco_evaluation.py:237] Performing evaluation on 600 images.\n","creating index...\n","index created!\n","INFO:tensorflow:Loading and preparing annotation results...\n","I0717 15:24:22.415644 140672979662592 coco_tools.py:116] Loading and preparing annotation results...\n","INFO:tensorflow:DONE (t=0.01s)\n","I0717 15:24:22.424231 140672979662592 coco_tools.py:138] DONE (t=0.01s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=1.15s).\n","Accumulating evaluation results...\n","DONE (t=0.15s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.104\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.188\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.096\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.138\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.264\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.316\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.328\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.436\n","INFO:tensorflow:Finished evaluation at 2020-07-17-15:24:23\n","I0717 15:24:23.812749 140674921572224 evaluation.py:275] Finished evaluation at 2020-07-17-15:24:23\n","INFO:tensorflow:Saving dict for global step 132: DetectionBoxes_Precision/mAP = 0.103539936, DetectionBoxes_Precision/mAP (large) = 0.137596, DetectionBoxes_Precision/mAP (medium) = 2.2264778e-06, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.18785559, DetectionBoxes_Precision/mAP@.75IOU = 0.09607863, DetectionBoxes_Recall/AR@1 = 0.2642458, DetectionBoxes_Recall/AR@10 = 0.31620112, DetectionBoxes_Recall/AR@100 = 0.32835194, DetectionBoxes_Recall/AR@100 (large) = 0.43643123, DetectionBoxes_Recall/AR@100 (medium) = 0.0021582735, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 5.129145, Loss/localization_loss = 2.390465, Loss/regularization_loss = 1.0964354, Loss/total_loss = 8.616044, global_step = 132, learning_rate = 0.003, loss = 8.616044\n","I0717 15:24:23.813069 140674921572224 estimator.py:2049] Saving dict for global step 132: DetectionBoxes_Precision/mAP = 0.103539936, DetectionBoxes_Precision/mAP (large) = 0.137596, DetectionBoxes_Precision/mAP (medium) = 2.2264778e-06, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.18785559, DetectionBoxes_Precision/mAP@.75IOU = 0.09607863, DetectionBoxes_Recall/AR@1 = 0.2642458, DetectionBoxes_Recall/AR@10 = 0.31620112, DetectionBoxes_Recall/AR@100 = 0.32835194, DetectionBoxes_Recall/AR@100 (large) = 0.43643123, DetectionBoxes_Recall/AR@100 (medium) = 0.0021582735, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 5.129145, Loss/localization_loss = 2.390465, Loss/regularization_loss = 1.0964354, Loss/total_loss = 8.616044, global_step = 132, learning_rate = 0.003, loss = 8.616044\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 132: training/model.ckpt-132\n","I0717 15:24:23.815936 140674921572224 estimator.py:2109] Saving 'checkpoint_path' summary for global step 132: training/model.ckpt-132\n","INFO:tensorflow:Saving checkpoints for 193 into training/model.ckpt.\n","I0717 15:32:45.961417 140674921572224 basic_session_run_hooks.py:606] Saving checkpoints for 193 into training/model.ckpt.\n","WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7ff0f0da04e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n","W0717 15:32:47.555009 140674921572224 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7ff0f0da04e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7ff0f16dc730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","W0717 15:32:48.198498 140674921572224 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7ff0f16dc730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","INFO:tensorflow:Calling model_fn.\n","I0717 15:32:48.752415 140674921572224 estimator.py:1148] Calling model_fn.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:32:51.191535 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:32:51.229188 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:32:51.265267 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:32:51.302663 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:32:51.339834 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:32:51.377605 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:Done calling model_fn.\n","I0717 15:32:53.015608 140674921572224 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2020-07-17T15:32:53Z\n","I0717 15:32:53.035187 140674921572224 evaluation.py:255] Starting evaluation at 2020-07-17T15:32:53Z\n","INFO:tensorflow:Graph was finalized.\n","I0717 15:32:53.480824 140674921572224 monitored_session.py:240] Graph was finalized.\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-193\n","I0717 15:32:53.482690 140674921572224 saver.py:1284] Restoring parameters from training/model.ckpt-193\n","INFO:tensorflow:Running local_init_op.\n","I0717 15:32:54.570128 140674921572224 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0717 15:32:54.760649 140674921572224 session_manager.py:502] Done running local_init_op.\n","INFO:tensorflow:Performing evaluation on 600 images.\n","I0717 15:34:28.243385 140672979662592 coco_evaluation.py:237] Performing evaluation on 600 images.\n","creating index...\n","index created!\n","INFO:tensorflow:Loading and preparing annotation results...\n","I0717 15:34:28.244931 140672979662592 coco_tools.py:116] Loading and preparing annotation results...\n","INFO:tensorflow:DONE (t=0.01s)\n","I0717 15:34:28.250908 140672979662592 coco_tools.py:138] DONE (t=0.01s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=1.19s).\n","Accumulating evaluation results...\n","DONE (t=0.15s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.107\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.192\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.100\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.143\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.265\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.314\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.330\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.438\n","INFO:tensorflow:Finished evaluation at 2020-07-17-15:34:29\n","I0717 15:34:29.675486 140674921572224 evaluation.py:275] Finished evaluation at 2020-07-17-15:34:29\n","INFO:tensorflow:Saving dict for global step 193: DetectionBoxes_Precision/mAP = 0.10744995, DetectionBoxes_Precision/mAP (large) = 0.14301284, DetectionBoxes_Precision/mAP (medium) = 3.6179015e-06, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.19218206, DetectionBoxes_Precision/mAP@.75IOU = 0.09983208, DetectionBoxes_Recall/AR@1 = 0.26452515, DetectionBoxes_Recall/AR@10 = 0.31438547, DetectionBoxes_Recall/AR@100 = 0.3297486, DetectionBoxes_Recall/AR@100 (large) = 0.43828997, DetectionBoxes_Recall/AR@100 (medium) = 0.0021582735, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 5.1325665, Loss/localization_loss = 2.3854184, Loss/regularization_loss = 1.0937319, Loss/total_loss = 8.611713, global_step = 193, learning_rate = 0.003, loss = 8.611713\n","I0717 15:34:29.675800 140674921572224 estimator.py:2049] Saving dict for global step 193: DetectionBoxes_Precision/mAP = 0.10744995, DetectionBoxes_Precision/mAP (large) = 0.14301284, DetectionBoxes_Precision/mAP (medium) = 3.6179015e-06, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.19218206, DetectionBoxes_Precision/mAP@.75IOU = 0.09983208, DetectionBoxes_Recall/AR@1 = 0.26452515, DetectionBoxes_Recall/AR@10 = 0.31438547, DetectionBoxes_Recall/AR@100 = 0.3297486, DetectionBoxes_Recall/AR@100 (large) = 0.43828997, DetectionBoxes_Recall/AR@100 (medium) = 0.0021582735, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 5.1325665, Loss/localization_loss = 2.3854184, Loss/regularization_loss = 1.0937319, Loss/total_loss = 8.611713, global_step = 193, learning_rate = 0.003, loss = 8.611713\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 193: training/model.ckpt-193\n","I0717 15:34:29.678767 140674921572224 estimator.py:2109] Saving 'checkpoint_path' summary for global step 193: training/model.ckpt-193\n","INFO:tensorflow:global_step/sec: 0.0972469\n","I0717 15:35:36.096773 140674921572224 basic_session_run_hooks.py:692] global_step/sec: 0.0972469\n","INFO:tensorflow:loss = 8.593774, step = 201 (1028.310 sec)\n","I0717 15:35:36.097918 140674921572224 basic_session_run_hooks.py:260] loss = 8.593774, step = 201 (1028.310 sec)\n","INFO:tensorflow:Saving checkpoints for 254 into training/model.ckpt.\n","I0717 15:42:51.298995 140674921572224 basic_session_run_hooks.py:606] Saving checkpoints for 254 into training/model.ckpt.\n","WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7ff0f28f13c8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n","W0717 15:42:52.919923 140674921572224 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7ff0f28f13c8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7ff0f27dcea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","W0717 15:42:53.132948 140674921572224 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7ff0f27dcea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","INFO:tensorflow:Calling model_fn.\n","I0717 15:42:53.723856 140674921572224 estimator.py:1148] Calling model_fn.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:42:56.578783 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:42:56.617921 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:42:56.655427 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:42:56.695044 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:42:56.732642 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0717 15:42:56.770547 140674921572224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:Done calling model_fn.\n","I0717 15:42:58.370254 140674921572224 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2020-07-17T15:42:58Z\n","I0717 15:42:58.389107 140674921572224 evaluation.py:255] Starting evaluation at 2020-07-17T15:42:58Z\n","INFO:tensorflow:Graph was finalized.\n","I0717 15:42:58.837176 140674921572224 monitored_session.py:240] Graph was finalized.\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-254\n","I0717 15:42:58.838902 140674921572224 saver.py:1284] Restoring parameters from training/model.ckpt-254\n","INFO:tensorflow:Running local_init_op.\n","I0717 15:42:59.962523 140674921572224 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0717 15:43:00.172255 140674921572224 session_manager.py:502] Done running local_init_op.\n","INFO:tensorflow:Performing evaluation on 600 images.\n","I0717 15:44:33.562168 140672988055296 coco_evaluation.py:237] Performing evaluation on 600 images.\n","creating index...\n","index created!\n","INFO:tensorflow:Loading and preparing annotation results...\n","I0717 15:44:33.564636 140672988055296 coco_tools.py:116] Loading and preparing annotation results...\n","INFO:tensorflow:DONE (t=0.01s)\n","I0717 15:44:33.572909 140672988055296 coco_tools.py:138] DONE (t=0.01s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=1.18s).\n","Accumulating evaluation results...\n","DONE (t=0.15s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.141\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.234\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.139\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.187\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.265\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.315\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.331\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.439\n","INFO:tensorflow:Finished evaluation at 2020-07-17-15:44:34\n","I0717 15:44:34.986815 140674921572224 evaluation.py:275] Finished evaluation at 2020-07-17-15:44:34\n","INFO:tensorflow:Saving dict for global step 254: DetectionBoxes_Precision/mAP = 0.14073557, DetectionBoxes_Precision/mAP (large) = 0.18689469, DetectionBoxes_Precision/mAP (medium) = 2.6978175e-06, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.23423652, DetectionBoxes_Precision/mAP@.75IOU = 0.1391791, DetectionBoxes_Recall/AR@1 = 0.2650838, DetectionBoxes_Recall/AR@10 = 0.31452513, DetectionBoxes_Recall/AR@100 = 0.33058658, DetectionBoxes_Recall/AR@100 (large) = 0.4394052, DetectionBoxes_Recall/AR@100 (medium) = 0.0021582735, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 5.124149, Loss/localization_loss = 2.3805532, Loss/regularization_loss = 1.0911206, Loss/total_loss = 8.595814, global_step = 254, learning_rate = 0.003, loss = 8.595814\n","I0717 15:44:34.987154 140674921572224 estimator.py:2049] Saving dict for global step 254: DetectionBoxes_Precision/mAP = 0.14073557, DetectionBoxes_Precision/mAP (large) = 0.18689469, DetectionBoxes_Precision/mAP (medium) = 2.6978175e-06, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.23423652, DetectionBoxes_Precision/mAP@.75IOU = 0.1391791, DetectionBoxes_Recall/AR@1 = 0.2650838, DetectionBoxes_Recall/AR@10 = 0.31452513, DetectionBoxes_Recall/AR@100 = 0.33058658, DetectionBoxes_Recall/AR@100 (large) = 0.4394052, DetectionBoxes_Recall/AR@100 (medium) = 0.0021582735, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 5.124149, Loss/localization_loss = 2.3805532, Loss/regularization_loss = 1.0911206, Loss/total_loss = 8.595814, global_step = 254, learning_rate = 0.003, loss = 8.595814\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 254: training/model.ckpt-254\n","I0717 15:44:34.992578 140674921572224 estimator.py:2109] Saving 'checkpoint_path' summary for global step 254: training/model.ckpt-254\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jz11BaypTLqF","colab_type":"text"},"source":["### Exporting The Trained model"]},{"cell_type":"code","metadata":{"id":"8d9-_fkPS7YO","colab_type":"code","colab":{}},"source":["#the location where the exported model will be saved in.\n","output_directory = '/content/gun_detection/models/research/fine_tuned_model'\n","\n","# goes through the model is the training/ dir and gets the last one.\n","# you could choose a specfic one instead of the last\n","lst = os.listdir(model_dir)\n","lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n","steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n","last_model = lst[steps.argmax()].replace('.meta', '')\n","last_model_path = os.path.join(model_dir, last_model)\n","print(last_model_path)\n","\n","#exports the model specifed and inference graph\n","!python /content/gun_detection/models/research/object_detection/export_inference_graph.py \\\n","    --input_type=image_tensor \\\n","    --pipeline_config_path={model_pipline} \\\n","    --output_directory={output_directory} \\\n","    --trained_checkpoint_prefix={last_model_path}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hABq0JAcTQnv","colab_type":"code","colab":{}},"source":["#downloads the frozen model that is needed for inference\n","files.download(output_directory + '/frozen_inference_graph.pb')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Neq1pYMJTUQS","colab_type":"code","colab":{}},"source":["#downlaod the label map\n","files.download(DATA_BASE_PATH + '/label_map.pbtxt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r5311XKsTV-y","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}